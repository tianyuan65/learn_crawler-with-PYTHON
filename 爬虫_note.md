## Urllib
## 一、 什么是互联网爬虫？
* 如果我们把互联网那个比作一张蜘蛛网，那一台计算机上的数据就是蜘蛛网上的一个猎物，而爬虫程序就是一只蜘蛛，沿着蛛网抓取自己想要的数据。
* 解释1：通过一个程序，根据地址/域名/url(https://jd.com/)进行爬取网页，获取有用信息。
* 解释2：使用程序模拟浏览器，去向服务器发送请求，获取响应信息。
## 二、爬虫核心
* 2.1 爬取网页：爬取整个网页，包含了网页中的所有内容。
* 2.2 解析数据：将从网页中得到的数据进行解析，从中获取想要的数据。
* 2.3 难点：爬虫和反爬虫之间的博弈，爬虫就是向服务器获取数据，反爬虫就是服务器拒绝提供数据。但反爬也是有原因的，这个问题可以解决。
## 三、爬虫的用途
* 数据分析/人工数据集
* 社交软件冷启动，有一些社交软件爬取了其他社交软件(如微博)的用户数据，作为自家软件的假数据。
* 舆情监控，爬取某一时段的人流量或天气状况有关的数据后，可以分解人流量和控制灾情。
* 竞争对象监控
## 四、 爬虫分类
* 通用爬虫：我不学
  * 实例：百度，360，google，sougou等搜索引擎
  * 功能：访问网页->抓取数据->数据存储->数据处理->提供检索服务
  * robots协议：一个约定俗成的协议，添加robots.txt文件，来说明本网站的那些内容不可以被抓取，起不到限制作用，自己写的爬虫无需遵守。
  * 网站排名(SEO)
    * 1. 根据pagerank算法值进行排名(参考网站流量，点击率等指标)
    * 2. 百度竞价排名
  * 缺点：
    * 1. 抓取的数据大多是无用的
    * 2. 不能根据用户的需求来精准获取数据
* 聚焦爬虫：我会以及要学
  * 功能：根据需求，实现爬虫程序，抓取需要的数据
  * 设计思路：
    * 1. 确定爬取的url。如何获取url？
    * 2. 模拟浏览器通过http协议访问url，获取服务器返回的htm代码。如何访问？
    * 3. 解析html字符串(根据一定规则提取需要的数据)。如何解析？
## 五、 反爬手段
* 5.1 User-Agent
  * User-Agent，中文名为用户代理，简称UA，它是一个特殊字符串头，使服务器能够识别客户使用的操作系统及版本，CPU类型、浏览器及其版本、浏览器渲染引擎、llq语言、浏览器插件等。
* 5.2 代理IP：若有异于人类基本操作的行为，服务器方就会封IP。
  * 西次代理
  * 快代理
  * 什么是高匿名、匿名和透明代理？它们之间有何区别？
    * 1. 使用透明代理，对方服务器可以知道你使用了代理，并且也知道你的真实IP。
    * 2. 使用匿名代理，对方服务器可以知道你使用了代理，但不知道你的真实IP。
    * 3. 使用高匿名代理，对方服务器不知道你使用了代理，更不知道你的真实IP。
* 5.3 验证码访问，就是为了防止爬虫，但都不是事。
  * 打码平台
  * 云打码平台
  * 超级
* 5.4 动态加载网页，网站返回的是js数据，并不是网页的真实数据，但也都不是大问题。
  * selenium驱动真实的浏览器发送请求
* 5.5 数据加密
  * 分析js代码
## 六、 urllib库的使用
* urllib库，该库不需要安装，因为它是Python本身自带的，不像其他的库，需要去官网手动安装。
* 主要是学习如何使用urllib来模拟浏览器，来访问服务器的数据。
  * 
## 七、 请求对象的定制
## 八、 编解码
* 8.1 get请求方式：urllib.parse.quote()
* 8.2 get请求方式：urllib.parse.urlencode()
* 8.3 post请求方式
## 九、 ajax的get请求
## 十、 ajax的post请求
## 十一、 复杂get
## 十二、 URLError\HTTPError
## 十三、 cookie登录
## 十四、 Handler处理器
## 十五、 代理服务器
## 十六、 cookie库