## Urllib
## 一、 什么是互联网爬虫？
* 如果我们把互联网那个比作一张蜘蛛网，那一台计算机上的数据就是蜘蛛网上的一个猎物，而爬虫程序就是一只蜘蛛，沿着蛛网抓取自己想要的数据。
* 解释1：通过一个程序，根据地址/域名/url(https://jd.com/)进行爬取网页，获取有用信息。
* 解释2：使用程序模拟浏览器，去向服务器发送请求，获取响应信息。
## 二、爬虫核心
* 2.1 爬取网页：爬取整个网页，包含了网页中的所有内容。
* 2.2 解析数据：将从网页中得到的数据进行解析，从中获取想要的数据。
* 2.3 难点：爬虫和反爬虫之间的博弈，爬虫就是向服务器获取数据，反爬虫就是服务器拒绝提供数据。但反爬也是有原因的，这个问题可以解决。
## 三、爬虫的用途
* 数据分析/人工数据集
* 社交软件冷启动，有一些社交软件爬取了其他社交软件(如微博)的用户数据，作为自家软件的假数据。
* 舆情监控，爬取某一时段的人流量或天气状况有关的数据后，可以分解人流量和控制灾情。
* 竞争对象监控
## 四、 爬虫分类
* 通用爬虫：我不学
  * 实例：百度，360，google，sougou等搜索引擎
  * 功能：访问网页->抓取数据->数据存储->数据处理->提供检索服务
  * robots协议：一个约定俗成的协议，添加robots.txt文件，来说明本网站的那些内容不可以被抓取，起不到限制作用，自己写的爬虫无需遵守。
  * 网站排名(SEO)
    * 1. 根据pagerank算法值进行排名(参考网站流量，点击率等指标)
    * 2. 百度竞价排名
  * 缺点：
    * 1. 抓取的数据大多是无用的
    * 2. 不能根据用户的需求来精准获取数据
* 聚焦爬虫：我会以及要学
  * 功能：根据需求，实现爬虫程序，抓取需要的数据
  * 设计思路：
    * 1. 确定爬取的url。如何获取url？
    * 2. 模拟浏览器通过http协议访问url，获取服务器返回的htm代码。如何访问？
    * 3. 解析html字符串(根据一定规则提取需要的数据)。如何解析？
## 五、 反爬手段
* 5.1 User-Agent
  * User-Agent，中文名为用户代理，简称UA，它是一个特殊字符串头，使服务器能够识别客户使用的操作系统及版本，CPU类型、浏览器及其版本、浏览器渲染引擎、llq语言、浏览器插件等。
* 5.2 代理IP：若有异于人类基本操作的行为，服务器方就会封IP。
  * 西次代理
  * 快代理
  * 什么是高匿名、匿名和透明代理？它们之间有何区别？
    * 1. 使用透明代理，对方服务器可以知道你使用了代理，并且也知道你的真实IP。
    * 2. 使用匿名代理，对方服务器可以知道你使用了代理，但不知道你的真实IP。
    * 3. 使用高匿名代理，对方服务器不知道你使用了代理，更不知道你的真实IP。
* 5.3 验证码访问，就是为了防止爬虫，但都不是事。
  * 打码平台
  * 云打码平台
  * 超级
* 5.4 动态加载网页，网站返回的是js数据，并不是网页的真实数据，但也都不是大问题。
  * selenium驱动真实的浏览器发送请求
* 5.5 数据加密
  * 分析js代码
## 六、 urllib库的使用
* urllib库，该库不需要安装，因为它是Python本身自带的，不像其他的库，需要去官网手动安装。
* 主要是学习如何使用urllib来模拟浏览器，来访问服务器的数据。在这里向百度发送请求，获取百度首页的源码，并从其中获取真正想要的数据
  * 首先导入urllib库下的request，
    * ```import urllib.request```
  * 其次，定义一个变量url，用于存放要访问的地址，在这里就是百度首页的地址
    * ```url='http://www.baidu.com'```
  * 再次，就可以向浏览器发送请求，发送请求的方法为urlopen方法，将要访问的地址作为参数传进去，即可向对应的服务器发送请求并接收服务器的反馈。
    * ```response=urllib.request.urlopen(url)```
    * ![urlopen方法，向服务器访问请求时调用的方法，将地址作为参数传进去即可](imgs/urlopen%E6%96%B9%E6%B3%95%EF%BC%8C%E5%90%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%BF%E9%97%AE%E8%AF%B7%E6%B1%82%E6%97%B6%E8%B0%83%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B0%86%E5%9C%B0%E5%9D%80%E4%BD%9C%E4%B8%BA%E5%8F%82%E6%95%B0%E4%BC%A0%E8%BF%9B%E5%8E%BB%E5%8D%B3%E5%8F%AF.png)
  * 再次，打印接收的数据response可以发现，是有响应头、响应体和响应行等的全部数据，但需要的只是相应当中的页面的源码，因此需要调用read方法来提取页面源码，再打印可以看到输出的是字节形式的二进制数据。
    * ![这个b代表，返回的是字节形式的二进制数据](imgs/%E8%BF%99%E4%B8%AAb%E4%BB%A3%E8%A1%A8%EF%BC%8C%E8%BF%94%E5%9B%9E%E7%9A%84%E6%98%AF%E5%AD%97%E8%8A%82%E5%BD%A2%E5%BC%8F%E7%9A%84%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%95%B0%E6%8D%AE.png)
  * 因此在下一步需要做的，就是将这些二进制数据转换为字符串类型的数据，这一步叫做解码，解码方法为decode()，参数位置传对应页面的编码格式，在这里就是utf-8，找编码格式的话，就是charset的值。最后打印时可以发现，打印出来的结果最前面没有**b'**了，也可以看见汉字了，就意味着解码成功。
    * ![调用decode方法并用utf-8的编码格式转换后，可以发现b没了，且可以看见汉字了，说明解码成功了](imgs/%E8%B0%83%E7%94%A8decode%E6%96%B9%E6%B3%95%E5%B9%B6%E7%94%A8utf-8%E7%9A%84%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2%E5%90%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%8F%91%E7%8E%B0b%E6%B2%A1%E4%BA%86%EF%BC%8C%E4%B8%94%E5%8F%AF%E4%BB%A5%E7%9C%8B%E8%A7%81%E6%B1%89%E5%AD%97%E4%BA%86%EF%BC%8C%E8%AF%B4%E6%98%8E%E8%A7%A3%E7%A0%81%E6%88%90%E5%8A%9F%E4%BA%86.png)
## 七、 请求对象的定制
## 八、 编解码
* 8.1 get请求方式：urllib.parse.quote()
* 8.2 get请求方式：urllib.parse.urlencode()
* 8.3 post请求方式
## 九、 ajax的get请求
## 十、 ajax的post请求
## 十一、 复杂get
## 十二、 URLError\HTTPError
## 十三、 cookie登录
## 十四、 Handler处理器
## 十五、 代理服务器
## 十六、 cookie库
https://github.com/tianyuan65/learn_crawler-with-PYTHON.git